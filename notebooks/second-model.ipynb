{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data\"\n",
    "ABUNDANCE_FILE = \"abundance_with_unique_diff.csv\"\n",
    "ABUNDANCE_START = 212\n",
    "\n",
    "OUT_FILE = '../logs/results-individual.json'\n",
    "\n",
    "data = pd.read_csv(os.path.join(DATA_FOLDER, ABUNDANCE_FILE), low_memory=False)\n",
    "\n",
    "dataset_names = data['dataset_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = {}\n",
    "\n",
    "# Remove spaces in the content of the 'disease' column\n",
    "data['disease'] = data['disease'].str.strip()\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    data_split[dataset_name] = data[data['dataset_name'] == dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_info = {\n",
    "    'Zeller_fecal_colorectal_cancer' : {'positive': ['cancer', 'small_adenoma', 'large_adenoma'], 'negative': ['n']},\n",
    "    'WT2D': {'positive': ['t2d', 'impaired_glucose_tolerance'], 'negative': ['n']},\n",
    "    'VerticalTransmissionPilot': {'positive': [], 'negative': ['nd']},\n",
    "    't2dmeta_short': {'positive': ['t2d'], 'negative': ['n']},\n",
    "    't2dmeta_long': {'positive': ['t2d'], 'negative': ['n', '-']},\n",
    "    'Tito_subsistence_gut': {'positive': ['overweight', 'obese', 'underweight'], 'negative': ['n']},\n",
    "    'Segre_Human_Skin': {'positive': [], 'negative': ['n']},\n",
    "    'Quin_gut_liver_cirrhosis' : {'positive': ['cirrhosis'], 'negative': ['n']},\n",
    "    'Psoriasis_2014': {'positive': ['y'], 'negative': ['n']},\n",
    "    'Neilsen_genome_assembly': {'positive': ['ibd_ulcerative_colitis', 'ibd_crohn_disease'], 'negative': ['n', 'n_relative']},\n",
    "    'metahit': {'positive': ['ibd_ulcerative_colitis', 'ibd_crohn_disease'], 'negative': ['n']},\n",
    "    'Loman2013_EcoliOutbreak_DNA_MiSeq': {'positive': ['stec2-positive'], 'negative': []},\n",
    "    'Loman2013_EcoliOutbreak_DNA_HiSeq': {'positive': ['stec2-positive'], 'negative': ['-']},\n",
    "    'hmpii': {'positive': [], 'negative': ['n']},\n",
    "    'hmp': {'positive': [], 'negative': ['n']},\n",
    "    'Chatelier_gut_obesity': {'positive': ['obesity'], 'negative': ['n', 'leaness']},\n",
    "    'Candela_Africa': {'positive': [], 'negative': ['n']},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subsets = {}\n",
    "\n",
    "for disease in disease_info.keys():\n",
    "    data_subset = data_split[disease]\n",
    "    \n",
    "    positive = pd.DataFrame()\n",
    "    for pos in disease_info[disease]['positive']:\n",
    "        subset = data_subset[data_subset['disease'] == pos]\n",
    "        positive = pd.concat((positive, subset))\n",
    "        \n",
    "    negative = pd.DataFrame()\n",
    "    for neg in disease_info[disease]['negative']:\n",
    "        subset = data_subset[data_subset['disease'] == neg]\n",
    "        negative = pd.concat((negative, subset))\n",
    "        \n",
    "    data_subsets[disease] = {'positive': positive, 'negative': negative}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_FOLDER_DIFF = \"../images/diff_features\"\n",
    "\n",
    "os.makedirs(IMAGES_FOLDER_DIFF, exist_ok=True)\n",
    "\n",
    "def plot_barh(diff_mean_sorted, disease_name, n_count=20):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    top_features = diff_mean_sorted.index[:n_count]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(top_features, diff_mean_sorted[:n_count])\n",
    "    plt.xlabel('Difference in mean abundance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Top {n_count} features with highest mean difference for {disease_name} dataset')\n",
    "    plt.savefig(os.path.join(IMAGES_FOLDER_DIFF, f'{disease_name}_top_features.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def preprocess_data(disease_name, data_subsets):\n",
    "    data_subset = data_subsets[disease_name]\n",
    "    \n",
    "    positive_data = data_subset['positive']\n",
    "    negative_data = data_subset['negative']\n",
    "    \n",
    "    # Only for visualizing ...\n",
    "    \n",
    "    positive_abundance = positive_data.iloc[:, ABUNDANCE_START:]\n",
    "    negative_abundance = negative_data.iloc[:, ABUNDANCE_START:]\n",
    "    \n",
    "    positive_mean = positive_abundance.mean(axis=0)\n",
    "    negative_mean = negative_abundance.mean(axis=0)\n",
    "    \n",
    "    diff_mean = abs(positive_mean - negative_mean)\n",
    "    diff_mean_sorted = diff_mean.sort_values(ascending=False)\n",
    "    \n",
    "    if positive_data.shape[0] != 0 and negative_data.shape[0] != 0:\n",
    "        plot_barh(diff_mean_sorted, disease_name)\n",
    "    \n",
    "    return positive_data, negative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for disease_name in disease_info.keys():\n",
    "#     preprocess_data(disease_name, data_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class to load the data for the model\n",
    "\n",
    "class DiseaseDataset(Dataset):\n",
    "    def __init__(self, dataset_name, positive_data, negative_data, device):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.positive_data = positive_data\n",
    "        self.negative_data = negative_data\n",
    "        \n",
    "        self.data = pd.concat((positive_data, negative_data))\n",
    "        \n",
    "        self.labels = np.zeros(len(self.data))\n",
    "        self.labels[:len(positive_data)] = 1 # positive samples are labeled as 1\n",
    "        \n",
    "        self.features = self.data.iloc[:, ABUNDANCE_START:]\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_train_tensor = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32).to(self.device)\n",
    "        y_train_tensor = torch.tensor(self.labels[idx], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        return X_train_tensor, y_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, idim, odim, layers, batch_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        \n",
    "        for i, layer in enumerate(layers):\n",
    "            if i == 0:\n",
    "                self.layers.add_module(\"fc{}\".format(i), nn.Linear(idim, layer))\n",
    "            else:\n",
    "                self.layers.add_module(\"fc{}\".format(i), nn.Linear(layers[i-1], layer))\n",
    "            \n",
    "            if batch_norm:\n",
    "                self.layers.add_module(\"bn{}\".format(i), nn.BatchNorm1d(layer))\n",
    "            \n",
    "            self.layers.add_module(\"relu{}\".format(i), nn.ReLU())\n",
    "            \n",
    "        self.layers.add_module(\"fc{}\".format(len(layers)), nn.Linear(layers[-1], odim))\n",
    "        self.layers.add_module(\"softmax\", nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for a epoch\n",
    "\n",
    "def train(model: nn.Module, loader: DataLoader, optimizer: torch.optim, criterion: nn.Module):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for _, (X_train_tensor, y_train_tensor) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_train_tensor)\n",
    "        \n",
    "        loss = criterion(y_pred, y_train_tensor)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module):\n",
    "    model.eval()\n",
    "    true = []\n",
    "    pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, (X_train_tensor, y_train_tensor) in enumerate(loader):\n",
    "            y_pred = model(X_train_tensor)\n",
    "            \n",
    "            true.extend(y_train_tensor.cpu().numpy())\n",
    "            pred.extend(y_pred.argmax(dim=1).cpu().numpy())\n",
    "    \n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = {\n",
    "    'disease': disease_info.keys(),\n",
    "    'lr': [0.00001, 0.00005, 0.0001, 0.0005, 0.001],\n",
    "    'batch_size': [4, 8, 16, 32],\n",
    "    'layers': [[128, 64], [256, 128, 64], [512, 256, 128, 64]],\n",
    "    'n_epochs': [200, 300, 400]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_name, data_subsets, LAYERS, lr, batch_size, n_epochs):\n",
    "    \n",
    "    DiseaseDataset_obj = DiseaseDataset(dataset_name, data_subsets[dataset_name]['positive'], data_subsets[dataset_name]['negative'], device)\n",
    "    \n",
    "    train_size = int(0.8 * len(DiseaseDataset_obj))\n",
    "    test_size = len(DiseaseDataset_obj) - train_size\n",
    "    \n",
    "    train_dataset, test_dataset = random_split(DiseaseDataset_obj, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = MLP(DiseaseDataset_obj.features.shape[1], 2, LAYERS).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, loss)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {train_loss}')\n",
    "    \n",
    "    true_train, pred_train = evaluate(model, train_loader, loss)        \n",
    "    true, pred = evaluate(model, test_loader, loss)\n",
    "    \n",
    "    return true_train, pred_train, true, pred, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disease': 'Zeller_fecal_colorectal_cancer', 'lr': 1e-05, 'batch_size': 4, 'layers': [128, 64], 'n_epochs': 200}\n",
      "Already exists\n",
      "{'disease': 'Zeller_fecal_colorectal_cancer', 'lr': 1e-05, 'batch_size': 4, 'layers': [128, 64], 'n_epochs': 300}\n",
      "Epoch: 0, Loss: 18.442748606204987\n",
      "Epoch: 100, Loss: 15.202354073524475\n",
      "Epoch: 200, Loss: 15.150361210107803\n",
      "Train accuracy:  0.9626168224299065\n",
      "Test accuracy:  0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "# Get combinations of hyperparameters\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "hyper_parameters_values = list(itertools.product(*hyper_parameters.values()))\n",
    "\n",
    "results = []\n",
    "\n",
    "if(os.path.exists(OUT_FILE)):\n",
    "    with open(OUT_FILE, 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "for hyper_parameter_values in hyper_parameters_values:\n",
    "    hyper_parameter = dict(zip(hyper_parameters.keys(), hyper_parameter_values))\n",
    "    \n",
    "    print(hyper_parameter)\n",
    "    \n",
    "    # check if already exists\n",
    "    exists = False\n",
    "    for result in results:\n",
    "        if result['disease'] == hyper_parameter['disease'] and result['layers'] == hyper_parameter['layers'] and result['lr'] == hyper_parameter['lr'] and result['batch_size'] == hyper_parameter['batch_size'] and result['n_epochs'] == hyper_parameter['n_epochs']:\n",
    "            exists = True\n",
    "            print('Already exists')\n",
    "            break\n",
    "        \n",
    "    if exists:\n",
    "        continue\n",
    "    \n",
    "    disease = hyper_parameter['disease']\n",
    "    if len(data_subsets[disease]['positive']) == 0 or len(data_subsets[disease]['negative']) == 0:\n",
    "        continue\n",
    "    \n",
    "    true_train, pred_train, true, pred, train_loss = train_model(hyper_parameter['disease'], data_subsets, hyper_parameter['layers'], hyper_parameter['lr'], hyper_parameter['batch_size'], hyper_parameter['n_epochs'])\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true, pred, average='binary')\n",
    "    \n",
    "    results.append({\n",
    "        'disease': hyper_parameter['disease'],\n",
    "        'layers': hyper_parameter['layers'],\n",
    "        'lr': hyper_parameter['lr'],\n",
    "        'batch_size': hyper_parameter['batch_size'],\n",
    "        'n_epochs': hyper_parameter['n_epochs'],\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'train_accuracy': accuracy_score(true_train, pred_train),\n",
    "        'test_accuracy': accuracy_score(true, pred)\n",
    "    })\n",
    "    \n",
    "    print(\"Train accuracy: \", results[-1]['train_accuracy'])\n",
    "    print(\"Test accuracy: \", results[-1]['test_accuracy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUT_FILE, 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
